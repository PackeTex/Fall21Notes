\lecture{11}{Fri 17 Sep 2021 10:19}{Hadamard Matrices (4)}
\begin{recall}
	A matrix was regular if all row sums are equal.
\end{recall}
As it turns out, for regular real hadamard matrices regular also implies equal column sums.
\begin{proof}
	Let \(H\) be hadamard regular and \(n \times n\) with \(\sum_{i= 1}^{n} h_{i, j} = d\) for all \(j\).\\
	Then, note that \(Hj = dj\) with \(j = \begin{pmatrix} 1\\ \vdots\\ 1 \end{pmatrix}\). Hence, \(d\) is an eigenvalue and as \(H^{*}H = H H^{*}\), then we have that \(H^{*}H j = H^{*} d j\). Hence \[
	nI j &=  dH^{*}j \text{ by hadamardness}
\]
and as \(Ij = j\) we have that \(H^{*} j = \frac{n}{d} j \), hence \(\frac{n}{d}\) is an eigenvalue of \(H^{*}\), hence the row sums of \(H^{*}\) are all \(\frac{n}{d}\), and as \(H^{*}  = H^{T}\) for real \(H\), we see the column sums of \(H\) are \(\frac{n}{d}\).\\
Additionally, if \(d \neq 0\), then \(\sum_{i= 1}^{n} r_{i} \left( H \right)  = \sum_{i= 1}^{n} c_{i}\left( H \right) \), implying \(nd = n \cdot \frac{n}{d}\), hence \(n = d^2\) as we have proven earlier.\\
We have, of course, neglected the case where \(d = 0\). In this case we have that \(nj = \vec{0}\), but as \(n \neq 0\) by assumption, and \(cj \neq \vec{0}\) for \(c \neq 0\), we have a contradiction. Hence \(d \neq 0\). It is also true that the independence requirement of hadamard matrices implies this row sum cannot be \(0\).
\end{proof}
\begin{proposition}
	Suppose \(H\) is a \(n \times n\) matrix with entries \(\left| h_{i, j} \right|  = 1\) and singular values \(\sigma_1 = \sigma_2 = \ldots = \sigma_{n} = \sqrt{n} \). Then, \(H\) is hadamard.
\end{proposition}
\begin{proof}
	Recall from an earlier proposition, we know \(\sum_{i= 1}^{n} \sigma_{i}^2 = n^2\). Recall that a diagonal element of \(HH^{*}\) is \(b_{i, i} = \sum_{k= 1}^{n} a_{i, k} \cdot \overline{a_{i, k}} = \sum_{k= 1}^{n} \left| a_{i, k} \right| ^2 = n\) by construction. Hence, the diagonals are all \(b_{i, i} = n\) for all \(1\le i \le n\). Next, we wish to see if there are any \(0\) entries in \(H H^{*}\). Next, we take  a principal submatrix \(A_{i, j} = \begin{bmatrix} n&\overline{b_{i, j}}\\
	b_{i, j}&n\end{bmatrix} \) (note this is as \(HH^{*}\) will is hermitian, so we know opposing entries will be complex conjugates) Then, we see \(\lambda_{1}\left( A_{i, j} \right) = n + \left| b_{i, j} \right| \) and \(\lambda_2 \left( A_{i, j} \right) = n - \left| b_{i, j} \right| \).\\
	Now, we examine how the eigenvalues of a matrix and its principal sumbatrices are related. Let \(A\) be a \(n \times n\) hermitian matrix and \(A^{\prime}\) to be \(A\) with the \(i\)'th row and \(j\)'th column removed. Denoted the eigenvalues of \(A\) to be \(\lambda_1, \lambda_2, \ldots, \lambda_{n}\) in decreasing order and eigenvalues of \(A^{\prime}\) to be \(\lambda_1^{\prime} ,  \lambda_2^{\prime}, \ldots, \lambda_{n-1}^{\prime}\). Then, it is a theorem of Cauchy that \(\lambda_1 \ge \lambda_1^{\prime} \ge \lambda_2 \ge \lambda_2^{\prime} \ge \ldots \ge \lambda_{n-1}^{\prime} \ge \lambda_{n}\). Applying this again yields a matrix \(A^{\prime}^{\prime}\) with eigenvalues \(\lambda_1 \ge \lambda_1^{\prime} \ge \lambda_1^{\prime}^{\prime}\) and \(\lambda_{n-2}^{\prime\prime} \ge \lambda_{n-1}^{\prime} \ge \lambda_{n}\).\\
	Returning to our original construction yields \(\lambda_1 \left( HH^{*} \right)  \ge \lambda_1 \left( A_{i, j} \right)  \ge \lambda_2 \left( A_{i, j} \right) \ge \lambda_{n} \left( HH^{*} \right) \) and as \(\lambda_1 \left( HH^{*} \right)  = \sigma_1^2 = n\) and similarly, \(\lambda_{n} \left( HH^{*} \right) = \sigma_{n}^2 = n\), hence \(\lambda_1 \left( A_{i, j} \right)  = \lambda_2 \left( A_{i, j} \right)  = n\) implying \(b_{i, j} = 0\) for all \(j \neq i\) and \(b_{i, i} = n\) so \(HH^{*} = nI\).
\end{proof}
\begin{recall}
	For a matrix \(H\) which is hadamard and has entries \(h_{i, i} = \delta\) for all \(i\), then the matrix \(A = \frac{1}{n}\left( J - \delta H \right) \) is a square matrix with entries \(0, 1\) and all \(0\)s along the diagonal.
\end{recall}
\begin{proposition}
	If \(H\) is symmetric, then \(A\) is the adjacency matrix of a simple graph. If \(H\) is also regular with row sum \(d\), then \(A\) is the adjacency matrix of a SRG with
	\begin{align*}
		n&=n\\
k &= \frac{n - \epsilon \sqrt{n} }{2} \\
\lambda &= \frac{n - 2\epsilon \sqrt{n} }{4}\\
\mu &= \frac{n - 2 \epsilon \sqrt{n} }{4} \\
	\end{align*}
	where \(\epsilon = \left \{
		\begin{array}{11}
			-1, & \quad \delta d < 0 \\
			1, & \quad \delta d > 0
		\end{array}
		\right\).
It is of note that \(\delta d \neq 0\) as \(\delta =  \pm 1\) and \(d \neq 0\) by the earlier proof. Hence, \(\epsilon \sqrt{n}  = \delta d\)
\end{proposition}
\begin{proof}
First, we examine a few matrix products. Note that as \(Hj = d\), we have \(HJ = dJ\). Similarly, \(JH = dJ\) and of course \(H^2 = nI\).\\
Next, we examine \(A^2\). By definition
\begin{align*}
	A^2 &=  \frac{1}{4} \left( J - \delta H \right) ^2 \\
	    &= \frac{1}{4} \left( J^2 - 2J\delta J + \delta^2 H^2 \right)  \\
	    &= \frac{1}{4} \left( nJ -2\delta dJ + nI \right)  \\
	    &= \frac{1}{4} \left( n -2 \delta d  \right) J + \frac{1}{4} n I \\
	    &= \frac{1}{4}\left( n-2\delta d \right) \left( J - I \right)  + \frac{1}{4}\left( n - 2 \delta d \right) I + \frac{1}{4}nI \\
	    &= \frac{1}{4} \left( n - 2 \delta d \right) \left( J - I \right)  + \frac{n - \delta d}{2} I
.\end{align*}
Recalling our equation for the square of the adjacency matrix of a graph, \[
	A^2 = \left( \lambda - \mu \right) A + \mu \left( J - I \right)  + kI
\] yields \(\lambda = \mu\), \(\mu = \frac{n - 2 \delta d}{4} = \frac{n - 2 \epsilon \sqrt{n} }{4} = \lambda\)  and \(k = \frac{n - \delta d}{2} = \frac{n - \epsilon \sqrt{n} }{2}\).
\end{proof}
This is a test. This is another
